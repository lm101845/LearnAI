{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、认识自然语言处理\n",
    "### 2、掌握文本分词工具\n",
    "\n",
    "### 3、词提取算法介绍\n",
    "### 4、文本分类聚类处理\n",
    "### 5、深度学习的NLP\n",
    "### 6、一些进化结构的讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、认识自然语言处理（处理是为了理解）\n",
    "自然语言处理 (NLP) 使用机器学习来剖析文本的结构和含义。借助自然语言处理应用，组织可以分析文本并提取关于人物、地点和事件的信息，以更好地理解社交媒体内容的情感和客户对话。\n",
    "\n",
    "\n",
    "自然语言处理应用用于从非结构化文本数据中发掘洞见，并让您能够访问所提取出的信息，以生成有关这些数据的新的理解。您可以使用 Python、TensorFlow 和 PyTorch 构建自然语言处理示例。\n",
    "\n",
    "![title](NLP用途.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里有一个小问题：我们再做数据挖掘的时候和图像识别的时候，实际上我们是在对什么进行操作？数字（或者说矩阵）！是不是？那么怎么样才能让文本变成数字呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、掌握文本分词工具jieba https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /usr/local/bin/pip: bad interpreter: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: no such file or directory\n",
      "\u001b[31mERROR: Invalid requirement: 'jieba#安装jieba'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install jieba#安装jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持四种分词模式：\n",
    "\n",
    "精确模式，试图将句子最精确地切开，适合文本分析；\n",
    "\n",
    "全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n",
    "\n",
    "搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "\n",
    "paddle模式，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注。paddle模式使用需安装paddlepaddle-tiny，pip install paddlepaddle-tiny==1.6.1。目前paddle模式支持jieba v0.40及以上版本。jieba v0.40以下版本，请升级jieba，pip install jieba --upgrade 。PaddlePaddle官网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /usr/local/bin/pip: bad interpreter: /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python: no such file or directory\n",
      "Requirement already satisfied: paddlepaddle in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (1.15.0)\n",
      "Requirement already satisfied: paddle-bfloat==0.1.7 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (0.1.7)\n",
      "Requirement already satisfied: numpy>=1.13 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (1.19.5)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: decorator in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (4.4.2)\n",
      "Requirement already satisfied: astor in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (3.13.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (2.27.1)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from paddlepaddle) (7.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from protobuf<=3.20.0,>=3.1.0->paddlepaddle) (56.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.20.0->paddlepaddle) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.20.0->paddlepaddle) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.20.0->paddlepaddle) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests>=2.20.0->paddlepaddle) (1.25.10)\n"
     ]
    }
   ],
   "source": [
    "! pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "paddle.enable_static()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3w/772jr2311zsbz371z3v36rdw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddle Mode: 我/来到/北京清华大学\n",
      "Paddle Mode: 乒乓球/拍卖/完/了\n",
      "Paddle Mode: 中国科学技术大学\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.994 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "strs=[\"我来到北京清华大学\",\"乒乓球拍卖完了\",\"中国科学技术大学\"]\n",
    "for str in strs:\n",
    "    seg_list = jieba.cut(str,use_paddle=True) # 使用paddle模式\n",
    "    print(\"Paddle Mode: \" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2用户词典-用来解决某些特定词的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 李/ 小/ 福/ 是/ 创新办/ 主任/ 也/ 是/ 云计算/ 方面/ 的/ 专家/ ;/  / 什么/ 是/ 八一双鹿/ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import jieba\n",
    "jieba.load_userdict(\"userdict.txt\")\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.add_word('石墨烯')\n",
    "jieba.add_word('凱特琳')\n",
    "jieba.del_word('自定义词')\n",
    "jieba.del_word(\"李小福\")\n",
    "\n",
    "\n",
    "test_sent = (\n",
    "\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\n",
    "\"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\n",
    "\"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\"\n",
    ")\n",
    "seg_list1 = jieba.cut(\"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\")\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1、关键词的提取 （基于 TF-IDF 算法的关键词抽取）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "\n",
    "上述引用总结就是, 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。这也就是TF-IDF的含义。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "俄罗斯,武器,北约,弹药,提供,报道,玩火,军事援助,照会,外长\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "content = open(\"wenben.txt\", 'rb').read()\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=10,withWeight=False,allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "\"\"\"sentence, topK=20, withWeight=False, allowPOS=()\n",
    "sentence 为待提取的文本\n",
    "topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n",
    "\"\"\"\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2、关键词的提取 （基于 Text-Rank 算法的关键词抽取）\n",
    "TextRank 算法是一种用于文本的基于图的排序算法，通过把文本分割成若干组成单元（句子），构建节点连接图，用句子之间的相似度作为边的权重，通过循环迭代计算句子的TextRank值，最后抽取排名高的句子组合成文本摘要。本文介绍了抽取型文本摘要算法TextRank，并使用Python实现TextRank算法在多篇单领域文本数据中抽取句子组成摘要的应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "俄罗斯,武器,弹药,提供,国家,系统,火箭,北约,美国,海马\n"
     ]
    }
   ],
   "source": [
    "tags1 = jieba.analyse.textrank(content, topK=10, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))\n",
    "print(\",\".join(tags1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "武器,北约,弹药,提供,拉夫罗夫,8.2,这轮,NASAMS,15,155\n"
     ]
    }
   ],
   "source": [
    "content = open(\"wenben.txt\", 'rb').read()\n",
    "\n",
    "jieba.analyse.set_stop_words(\"stopwords.txt\")\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=10)\n",
    "\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 LOC\n",
      "天安门 LOC\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\") #jieba默认模式\n",
    "jieba.enable_paddle() #启动paddle模式。\n",
    "words = pseg.cut(\"我爱北京天安门\",use_paddle=True) #paddle模式\n",
    "for word, flag in words:\n",
    "   print('%s %s' % (word, flag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、词提取算法介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1、TF-IDF算法的介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF(词频)-IDF（逆文档频率）\n",
    " \n",
    "TF = 某个词在文章中出现的次数/文章中的总词数\n",
    "\n",
    "IDF = log（语料库中的文档总数/（包含该词的文档数+1））\n",
    "\n",
    "然后再就是按照降序排列，取前面的几个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['通过训练得到的词向量我们能够进行对应的自然语言处理工作。比方求相似词、关键词聚类等。',\n",
    "        '自然语言处理的课程很好呀，大家快点收藏，点赞！',\n",
    "        '波老师讲的这个自然语言处理课程真的很nice',\n",
    "        '原来自然语言处理课程学要这样子学呀？']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['关键词聚类等', '原来自然语言处理课程学要这样子学呀', '大家快点收藏', '比方求相似词', '波老师讲的这个自然语言处理课程真的很nice', '点赞', '自然语言处理的课程很好呀', '通过训练得到的词向量我们能够进行对应的自然语言处理工作']\n",
      "{'通过训练得到的词向量我们能够进行对应的自然语言处理工作': 7, '比方求相似词': 3, '关键词聚类等': 0, '自然语言处理的课程很好呀': 6, '大家快点收藏': 2, '点赞': 5, '波老师讲的这个自然语言处理课程真的很nice': 4, '原来自然语言处理课程学要这样子学呀': 1}\n",
      "[[0.57735027 0.         0.         0.57735027 0.         0.\n",
      "  0.         0.57735027]\n",
      " [0.         0.         0.57735027 0.         0.         0.57735027\n",
      "  0.57735027 0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vec.fit_transform(corpus)\n",
    "# 得到语料库所有不重复的词\n",
    "print(tfidf_vec.get_feature_names())\n",
    "# 得到每个单词对应的id值\n",
    "print(tfidf_vec.vocabulary_)\n",
    "# 得到每个句子所对应的向量，向量里数字的顺序是按照词语的id顺序来的\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
