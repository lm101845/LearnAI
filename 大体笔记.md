# 大体笔记

> https://www.zhihu.com/question/435059024/answer/2693905278

NLP领域的学习，我认为有三个特点：

1. 知识比较新。比如BERT，也就是近几年提出来的；
2. 知识比较繁杂。大概罗列一些，RNN&LSTM&seq2seq&transformer、word2vec/glove/fasttext、elmo&bert等等。
3. 深度学习的实现，不像机器学习那么开箱即用，需要更高的代码要求

## 统计学习，机器学习，深度学习

> https://zhuanlan.zhihu.com/p/379821665

[找到上帝的两种套路 – 聊聊统计学习和机器学习](https://link.zhihu.com/?target=https%3A//www.sohu.com/a/132297224_667634)

统计学习里最重要的两个部分就是回归分析和假设检验。其他的方法或者技术在统计学习这个大框架下，最终也是为了这两者服务的。回归分析提供了解释因果的武器，假设检验则给这项武器装上了弹药。

在统计推断理论中，有这么一种妥协- bias - variance tradeoff，指的是我们在寻找一个统计估计量来接近真实值时，考虑到所拥有数据的健康程度，在估计精度和估计可信度之间需要做出平衡。打个比方，要得到偏离度很小的估计量必然会牺牲一部分可信度。而如果看重可信度，希望可信度的区间尽可能小，那么我们得到的估计量可能会和真实值有较大偏差。

这也是我们在逼近上帝函数的过程中遇到的问题。统计推断重解释，机器学习重预测。在小样本下，逻辑回归作为基础的线性分类器预测效果通常不比神经网络和其他ensembled算法差，且解释能力更强。当数据量越大，神经网络的预测能力就越强大，类似回归的统计推断方法越力不从心。在样本量不大的情况下，我们往往会比较重视模型的解释能力，因为数据量有限，特征之间是否有共线性不难发现，特征选择也只是在较少的维度下进行，模型的预测能力在我们的可控范围内不难做到最好。而在样本量大到超过我们的可控范围的情况下，预测能力是我们更看重的。因为这时，特征选择和特征间的相关性检测超过了我们的能力并且会极大伤害到我们的运算速度，牺牲特征的相关性检测，间接地等于放弃了模型的解释能力。从这个角度来讲，现在这个阶段，我们并没有一个很好的同时满足上帝函数两个必要条件的并且逼近上帝函数的统计或机器学习模型。

[统计学习，机器学习与深度学习概念的关联与区别 | 刻骨铭心](https://link.zhihu.com/?target=http%3A//jacoxu.com/statisticandmachinelearning/)

简单总结来说：

\1. 机器学习与统计学习有较大的Overlab，或者说机器学习是建立在统计学习的基础之上；

\2. 统计学习是theory-driven，对数据分布进行假设，以强大的数学理论支撑解释因果，注重参数推断（Inference）；

\3. 机器学习是data-driven，依赖于大数据规模预测未来，弱化了收敛性问题，注重模型预测（Prediction）；

\4. 深度学习是机器学习的一个子领域，特征提取更依赖于隐层模型，解释性弱，趋于黑盒子。

具体阐述如下：

- **深度学习是机器学习的一个子领域**

一般来讲，主流观点认为机器学习包括深度学习，即基于神经网络的深度学习是机器学习的一种，如图1所示。这点大家没有什么质疑，那么统计学习与机器学习存在什么关联呢？是否为同一个概念呢。

![img](https://pic2.zhimg.com/80/v2-adaf96818cd053ae66abf9ddbe5e496d_1440w.webp)

图1

- **机器学习建立在统计学习基础之上**

李航老师在2012年出版过一本《统计学习方法》的图书，如图2所示。图书内容包含了朴素贝叶斯，支持向量机，隐马尔可夫模型等有监督学习方法，以及K均值、层次聚类等无监督学习方法。**可以看到与传统机器学习内容基本一致，**而且在今年第二版简介中提到“统计学习方法即机器学习方法，…”[1]。因此，**可以说统计学习在某种场合下与机器学习概念基本一致。统计学习与经典机器学习方法有较强的关联性。**2012年之前，机器学习领域学者入门的一本宝典是Trevor Hastie于2009年出版的《The Elements of Statistical Learning》。统计学专家Aleks Jakulin在Quora上针对“统计学习与机器学习区别[3]”的问题中简单答道：

**“机器学习是人工智能领域人员做数据分析，数据挖掘是数据库领域人员做数据分析，统计学习是统计学领域人员做数据分析”（反正就是做数据分析）**

获得最高的认可度。该问题下获得第二认可度的回答是来自斯坦福的统计学研究生Giuseppe Paleologo，同样认为：

“统计学习与机器学习本质上是一致的（Essentially Equivalent），一些显而易见的区别是，机器学习领域发展迅速，学术会议影响力大，**而统计学习注重理论推演**，学术期刊影响力大。机器学习专家来自CS/EE专业，而统计学习专家来自Stat/Math专业”。CMU统计学大神Larry Wasserman教授于2012年6月写过一篇博文《Statistics Versus Machine Learning》[4]简短的回答是：**两者没有什么区别，都是研究如何从数据中学习。目前这两个领域已经越来越同化，相互借鉴和启发想法。**

**然而，严谨地讲，**混淆机器学习和统计学习两个概念会被认为是一种过于简单的表述，不太合理。**主流更倾向于接受，机器学习方法是建立在统计学习基础之上的。**

- **寻找“上帝函数”的两种套路：理解和预测**

如下大部分观点来自甚至直接摘自[2]。不管是统计学习，机器学习还是深度学习，数据分析员、工程师以及科学家都在追求一个终极梦想：找到或者能够无限接近一个“上帝函数”- 一个能够完美利用数据解决现实各种问题的模型或者方法。

那么，这样的“上帝函数”有什么特点呢？我们说学习的目的无外乎两点：理解和预测，所以我们期盼它能拥有两个能力或者说两个必要条件：解释因果和预测未来**。我需要知道问题的本质是什么，形成我们看到的结果的推动力究竟是什么（我是谁？我从哪里来？），这是因果分析；未来事物的发展会怎么样，是否按照某一个模式（我要到哪里去？)，这是预测分析。**

- **统计学习以强大的数学理论支撑解释因果**

统计学习依托背后的数学理论，在远早于机器学习大爆发的这十年，率先从解释因果的能力的角度，努力寻找上帝函数。[2]的观点认为，统计学习里最重要的两个部分就是回归分析和假设检验。其他的方法或者技术在统计学习这个大框架下，最终也是为了这两者服务的。回归分析提供了解释因果的武器，假设检验则给这项武器装上了弹药。单纯的线性回归用最小二乘法求解逼近事实的真相，再使用显著性检验，检测变量的显著性、模型的显著性、模型的拟合精度。当然是否属于线性，也可以使用假设检验的方法检测。非线性回归的问题，使用极大似然估计或者偏最小二乘回归求解模型，后续的显著性检验仍然是一样的思路。显著性检验有它的局限性，这本身是由统计学习的一些限定假设引起的，在没有更强大的解释因果的方法框架出现前，它依然是解释因果的第一选择。虽然显得粗糙，但是能用。

从逻辑回归模型的角度来理解统计学习。从统计的方法论来看，逻辑回归脱胎于目标变量属于binary分布的非线性模型。所有的回归问题可以归结为确定稳定统计量（比如目标变量的期望或者中位值）和解释变量的函数关系。这种函数关系在目标变量服从指数分布族时，可以推导出它的结构，我们只需再求解结构中的未知参数即可。这种结构被称之为sigmoid函数，在信息学里面经常能用到。那么之所以从统计的角度逻辑回归**可以得到严谨的数学解释和推断，全依赖于服从分布这个强假设。在这个假设下发展出的一整套理论，提供了现在这个通过数据学习世界的初级阶段，最优的解释因果框架**。

- **机器学习依赖于大数据规模预测未来**

我们再从逻辑回归出发来看看机器学习。机器学习，连带属于它的深度学习或强化学习，天生是为了解决大数据下的预测能力而提出并且发展的。目前最火热的图像识别、语音识别，包括金融领域里的借贷风险识别，为机器学习的发展提供了极其丰富的土壤- **极其大的数据量和极其多的（且极其稀疏的）数据特征。而众所周知的是，传统的回归分析在处理这种场景下的问题，收敛速度和预测精度都无法达到满意的程度。**求解回归模型并非线性问题，在模型训练阶段只能在算法迭代过程中使用分布式系统提高运算速度，算法速度的提高受到限制。**高维数据中经常碰到的稀疏问题，在回归模型中需要大量的预处理，也很难保证算法最后的收敛和估计精度**。另一方面，逻辑回归可以看作是神经网络算法的一个特例- 删去隐含层，输入层和输出层直接用sigmoid函数连接。而加入了隐含层，通过参数调优，在原本逻辑回归的框架下，能够大大提高处理大样本，高稀疏的数据分类问题。虽然对于我们来说，**隐含层输出的特征依然类似于上帝的选择–** **机器根据训练过程中已有的输入和输出信息，得到了比人工选择更加优秀的数据特征，在最终的模型预测能力上大大胜过了原有的特征选择**。



- **统计学习与机器学习的优势与劣势**

在统计学习理论中，有这么一种妥协- bias – variance tradeoff，指的是我们在寻找一个统计估计量来接近真实值时，考虑到所拥有数据的健康程度，在估计精度和估计可信度之间需要做出平衡。打个比方，要得到偏离度很小的估计量必然会牺牲一部分可信度。而如果看重可信度，那么我们得到的估计量可能会和真实值有较大偏差。

这也是我们在逼近上帝函数的过程中遇到的问题。**统计推断（Inference）重解释，机器学习重预测（Prediction）**。在小样本下，逻辑回归作为基础的线性分类器预测效果通常不比神经网络和其他ensembled算法差，且解释能力更强。当数据量越大，神经网络的预测能力就越强大，类似回归的统计推断方法越力不从心。在样本量不大的情况下，我们往往会比较重视模型的解释能力，因为数据量有限，特征之间是否有相关性不难发现，特征选择也只是在较少的维度下进行，模型的预测能力在我们的可控范围内不难做到最好。而在样本量大到超过我们的可控范围的情况下，预测能力是我们更看重的。因为这时，特征选择和特征间的相关性检测超过了我们的能力并且会极大伤害到我们的运算速度，牺牲特征的相关性检测，间接地等于放弃了模型的解释能力。从这个角度来讲，**现在这个阶段，我们并没有一个很好的同时满足上帝函数两个必要条件的并且逼近上帝函数的统计或机器学习模型。**模型的选择没有最好的，只有最合适的。脱离业务背景和业务需求，单纯追求算法速度或者拟合精度，就都是舍本逐末。

- **深度学习与传统机器学习的对比**

相比传统机器学习，深度学习最直观区别是在加深了隐含层由模型自动抽取特征，参数量大，训练时间长，弱化了解释性，趋向于黑盒子，对数据依赖性更强，更擅长处理高维度大数据。

![img](https://pic3.zhimg.com/80/v2-518c170c9cf87cc5f7f6fe9324fe52b2_1440w.webp)

参考文献：

[1] 统计学习方法第二版（豆瓣）：[https://book.douban.com/subject/33437381/](https://link.zhihu.com/?target=https%3A//book.douban.com/subject/33437381/)

[2] 找到上帝的两种套路 – 聊聊统计学习和机器学习：[https://www.sohu.com/a/132297224_667634](https://link.zhihu.com/?target=https%3A//www.sohu.com/a/132297224_667634)

[3] 统计学习与机器学习区别 – [https://www.quora.com/What-is-the-difference-between-statistics-and-machine-learning](https://link.zhihu.com/?target=https%3A//www.quora.com/What-is-the-difference-between-statistics-and-machine-learning)﻿

[4] [http://normaldeviate.wordpress.com/2012/06/12/statistics-versus-machine-learning-5-2/](https://link.zhihu.com/?target=http%3A//normaldeviate.wordpress.com/2012/06/12/statistics-versus-machine-learning-5-2/)

[5] 机器学习与深度学习区别：[https://blog.csdn.net/thisinnoc](https://link.zhihu.com/?target=https%3A//blog.csdn.net/thisinnocence/article/details/81294600)

## 请问统计学习方法（李航）、机器学习（周志华）以及python的学习顺序是什么呢

> https://www.zhihu.com/question/525930095/answer/2448849305

## 自学NLP需要多长时间？

> https://www.zhihu.com/question/330994295/answer/1083296552

我不清楚别人。我给你说下我学NLP的背景吧。我是本科 cs 专业加硕士cs专业NLP方向目前是NLP博士在读。

1.数学基础课很重要

高等数学 线性代数 概率统计 三大数学基础课。推荐mit 的微积分公开课和台大教授李宏毅的线性代数课。概率统计相对简单，可以自己拿教材学一学，也可以网上找找公开课。

2.[计算机基础](https://www.zhihu.com/search?q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)也很重要

a. 至少学会两到三门编程语言 其中一定包括python

b. 数据结构和算法，操作系统，[计算机网络](https://www.zhihu.com/search?q=%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)，计算机系统组成四大基础课必学。数据库和[编译原理](https://www.zhihu.com/search?q=%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)推荐学习。这部分直接有利于之后写代码的时候找bug。因为出现问题后很多计算机知识就会纷至沓来带你找到原因而不是变成无头苍蝇。

3.机器学习基础

a. 李航教授的《统计学习方法》必学

b. [周志华](https://www.zhihu.com/search?q=%E5%91%A8%E5%BF%97%E5%8D%8E&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)教授的西瓜书推荐

c. 深度学习和[人工神经网络](https://www.zhihu.com/search?q=%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)必学

d. 各种深度学习框架如[tensorflow](https://www.zhihu.com/search?q=tensorflow&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)，pytorch

可以找网课学习比如斯坦福公开课，台大教授李宏毅的课，[吴恩达](https://www.zhihu.com/search?q=%E5%90%B4%E6%81%A9%E8%BE%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)的课

4.NLP 基础

推荐斯坦福224系列和何晗著的《[自然语言处理入门](https://www.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)》(ps：何晗之前是学日语转的nlp@hankcs)还有 很多大佬著的《speech and language processing》

5.其他

各种基础虽然很重要，但如果想快速上手还是要多写代码练习。github上有很多好项目可以去看看。**研究深入之后可以开始看行业论文。**

**整个过程快的话历时2-3年(必须起早搭黑的学)，慢的话至少5年，特别慢的话十年也不是没有可能（我从本科开始到今年也第8年了）。**当然如果只是想玩儿一玩儿那就无所谓了。所以[加油吧少年](https://www.zhihu.com/search?q=%E5%8A%A0%E6%B2%B9%E5%90%A7%E5%B0%91%E5%B9%B4&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A1083296552%7D)！

## 我的NLP学习之路

> https://zhuanlan.zhihu.com/p/64026730

应好友邀约，写一篇有关我在NLP上学习的历程，以供大家作参考。点开的朋友，如果真的想了解这个过程，也想知道一些具体的信息，并从中有所收获，真心希望大家能认真看下去，尤其是一些近期和我在聊的新人，我虽不算大神，但是自认为还是有一定经验的，如果信得过我，请认真看吧。如果有什么想法欢迎和我多聊~

懒人目录

- 学习历程
- 学习路径
- 学习建议
- 其他建议

### 学习历程

#### 背景介绍

为了更好地让大家去理解我的历程，我先说说我当时的背景。

时间起点就从大四开始吧，信息与计算科学（提醒一下，这是个数学类专业，不是计算机的）专业，数学理论上还是比较自信的，计算机会点皮毛吧，matlab会点，写过点Java和前端三剑客，python也是大四中后期才自学的，蹩脚得很。

就在这种情况下，开始机器学习和NLP方面的学习。

#### 启蒙阶段

大四期间（16-17学年）因为机缘巧合接触了**李航老师的《统计学习方法》**，并在团队合作的情况下通关了里面的内容，回想起来还是满满的求生欲。由于只是过了书上的内容，所以只是对理论有了比较完整的了解，但是具体怎么用，怎么实现，基本是不知道的。

这段时间，基本就是拼的自己的脑子在攻克这本书，实际上，对于数学专业来说，这本书相比微分方程、实分析之类的还是简单很多，所以数学出身的同学还是不用太担心，但是**对于高数、线代、概率论不好的同学，吃力是毋庸置疑的**。

当然的，书终究只是书，有时候书真的看不懂，我会结合着网上的别的材料材料一起看，博客之类的，可以帮助理解，当然有些会有些错误，包括书本身和博客，需要自己甄别，好在不太多，不至于有三观级别的查错。

很多人因为这本书的困难而放弃，而且很多人会非常追捧周志华老师的《机器学习》，诚然这是本好书，但是如果你因为《统计学习方法》看不懂而去看周志华老师的《机器学习》，那你后面的提升会被严重限制，例如你只知道HMM的三个问题却不知道这三个问题怎么解决。（因为难而躲避，你终究会为你的躲避付出代价，要想多赚点真不是说有就有的）。

有关方法实现，我的学习基本来自于百度，例如“python SVM”，这样就会出大量的文章来讲，选一个试试你就会了，这已经是非常简单的方法了。

懒人记录：

- 《统计学习方法》不仅在于里面的方法，还有里面的计算思想，正则化，极大似然估计，梯度下降等，所以理论搞懂，会为你以后的进步打下基础，欠的账迟早要还。
- 注意除了学方法，还要学实现，我当时欠了这块的账，在后续才补了回来，**自己找个数据集玩玩（鸢尾花）**之类的，做一遍就好
- Python是这块的基操，没有不会的理由。

#### 入门阶段

机器学习想必是有些基础了，但是如何用，在哪用，其实非常苦恼，因此自己结合自己的研究生方向与当前社会现状，和老师沟通了一条线路，NLP，主要课题是研究如何通过NLP与网络信息实现对金融市场的预测，这也就奠定了整个研究生阶段的主要内容了。可惜的是，老师其实对NLP的了解不深，只能从金融市场有关内容中给我建议，但是NLP既然要做，也注定是一条孤独艰难的路。

17年9月入学，至12月，我完成了第一篇有关研究内容的初稿，从什么都不知道，到有论文吧，这段时间除了偶尔看看LPL的比赛，基本就在学校理学楼221里面（学校一间几乎被废弃，桌子除了我的位置基本都是尘的教室里）自习，严格而言都不算实验室，只是一个可供自习落脚的地方吧，这么一个环境学习下来的。

当时完全不清楚NLP是什么，也不知道这东西怎么做，只能通过看论文、看博客慢慢去学，当时甚至连淘宝和网课都没有，前期每天只有两个工作，**刷论文和写爬虫**。爬虫那时候技术不是很强，用的自己比较熟悉的Ajax去网络抓包，**论文方面是每看一篇就整理成笔记，笔记多少和文章摘要作用之类的有关，觉得有用了才回去深看**，后来**一篇综述成了一个转折点**，这篇综述我至今仍然能口述，简直是救命的神药。论文虽然没有说具体的研究理论和方法，但是既然是综述，也就提到了很多名词，我也知道了要学什么，要做什么，目前的研究现状如何，因为这篇文章，我感觉我的瓶颈都解决了。

于是就开始**按图索骥，查找有关方法和理论**，慢慢把技术学起来，另外我还百度了一本大型著作——**《统计自然语言处理》**。从基本的语言模型，从简单的开始，慢慢开展（**我的第一个文本分类的基线就是从TFIDF+SVM开始的**），然后是，用着自己蹩脚的，现在看来简直是垃圾的代码技术，**想一行，搜一行地完成了这块完整的代码**，并通过各种实验最终完成了自己的第一稿论文（创新点啥的肯定都是有的），那时就是12月。

懒人记录：

- 别老说没灵感，艺术家如此，科研也如此，**灵感来源于你的积累，积累从哪里来，论文**。
- 动手查，动手做，多查资料，百度、论文、github，道理都懂，但是**为啥还有很多人会问我百度就能知道的问题呢？**
- 不是宣扬独立战斗，团队应该有团队的样子，但是你应该有独立解决问题的能力，否则团队要你干吗？会求助是好事，但是**要问到别人手把手教你的层次，那就是个巨婴了**。
- **综述是个好东西**，带survey、review的多半是，看看会对研究现状有清晰的了解，近期的关键论文基本都被引在里面了，你也省去很多找论文和鉴别论文的时间。
- 《统计自然语言处理》最好看看，虽然里面谈的方法不多，但是引文很全，甚至建议引文都看看，**简单过一遍至少你知道文本分析和自然语言处理的差别在哪里，别老是炒概念**。
- 从最简单的开始，先做一个初步的，然后开始慢慢加大难度，这应该是一个正常的学习历程。

#### 进阶阶段

说到进阶，大概就是从18年上半年开始吧。当时面临的一个大问题是怎么找工作，当然还有论文的推进，到了工业界，技能要求更完善，因此这块也是我自学的集中期，论文在坚持看但是重在基础的完善（**光看论文知识很难系统化、完善化**）。

深度学习方面，自己的深度学习是平时积累的，BP是本科的课程就已经接触，CNN之类的也是在平时找个1天左右看博客看会的，RNN是后来自己用到了才临时看的，包括后续的LSTM之类的，真没有集中专项学习。我目前会tensorflow和keras，前者是淘宝花了4块钱买的教程，keras是看文档自学的，后来买了一本紫色封面的书边看边学会的。

自己在文本分类方面看过不少论文，也有重现，所以有一些理解。建议大家还是把文本分类能自己动手把几个常见模型都看一遍（博客上、公众号上经常有人提到的，就叫做常见），毕竟文本分类是最最简单的自然语言处理任务，也是门槛最低的，这个都不会复杂的任务更加做不了对吧。

另外，为了自己对NLP目前现状有个更好地了解，我自己投资了一下，买了一门网课吧，选的是深蓝学院的课，其实几百块，每周2次，真不指望能学到什么（这么短时间就能学会为啥还有人感慨学不会呢？），但是能让我知道我有哪些是应该知道但是不知道的，然后开始学起来。

NLP的自学其实到这块就已经进入这个行业了，进入到行业里也就有了很多信息，过程的话我就写到这里吧。

#### 学习路径

上面说了很多，现在来总结一下我的学习路径，当时算是刀耕火种的时代，下面这个只能做参考。

- 《统计学习方法》
- 刷有关领域的论文，因为论文体现发展前沿的重要方式
- 《统计自然语言处理》
- 博客+论文级别的深度学习基础get
- 淘宝买的tensorflow视频，keras文档学keras
- 自学gensim等NLP工具+动手实践

由于现在资源比较丰富了，所以还是推荐大家看看下面的材料

- 百度、淘宝、github、论文，最快捷、扎实的学习路径
- 机器学习：《统计学习方法》、雷明的《机器学习与应用》，塞巴斯蒂安的《python机器学习》，sklearn的API文档建议自己多看看查查
- 深度学习：黄文坚《tensorflow实战》、tensorflow技术解析与实战
- 自然语言处理：《统计自然语言处理》、刘兵的《情感分析》、《基于深度学习的自然语言处理》。个人还是建议多看看博客、公众号等，经常提到的模型才是最好的，目前大部分书都只是浮于表面，这只与NLP这块的发展现状有关，博客多看自己也多动手
- 其他：《数据挖掘导论》，《机器学习实践指南》、《python数据分析与挖掘实战》、《精通数据科学》

从我的视角，这些书能够看完并且能够理解，超过60%-70%的人没问题，关键就看你能不能看下来。

### 学习建议

- 扎实基础，无论是数学还是编程开发，不要想着避开，出来混总是要还的
- 不要老是抱怨自己没有资料，只是你没找过而已，甚至是百度淘宝都是有的
- 不一定要系统学习，前沿知识很难有体系，所以一方面平时要接受信息，另一方面坚持去查资料看
- 多阅读工具给的文档，英文？翻译软件能帮到你

### 其他建议

- 这个领域对数学和技术都具有一定要求，门槛其实很高，当然赚的不少，但是有没有媒体说的那么好呢，我的答案是否，所以大家要谨慎，别就顾着炒概念和涌入，泡沫破了代价很大。
- 技术扎实是安身立命之本，裁员也裁水平不够的对吧
- 多！看！论！文！没灵感了看论文，实验不会做了参考论文，文章格式不会拉的时候看论文。
- 尽量尝试自己把问题解决，问别人之前多问问自己，百度了没，谷歌了没，看过论文没，都是怎么说的，我还没解决的点在那里，绝对比问一个人“在吗”然后抖着腿等高效多了

### 后记

没想过自己会通过这个基于回顾到整个自己自学的历程，研究生历程远不止如此，但是NLP的学习历程基本能覆盖，但是NLP的学习和进步历程远不止这些，由于自己的工作方向等原因，近期我还在看短文本分了等方面的内容，有新的进展会和大家分享，敬请期待。

